# ğŸ” RAG Pipeline

> *Retrieval-Augmented Generation â€” the AI's knowledge engine*

## ğŸ”„ How It Works
| Step | What happens |
|---|---|
| 1ï¸âƒ£ | ğŸ“ MCP `.md` files split into chunks |
| 2ï¸âƒ£ | ğŸ§¬ Each chunk embedded via Gemini `text-embedding-004` |
| 3ï¸âƒ£ | ğŸ“¦ Vectors stored in `meta.mcp_embeddings` (HNSW) |
| 4ï¸âƒ£ | ğŸ—£ï¸ User question embedded as vector |
| 5ï¸âƒ£ | ğŸ” Cosine similarity finds top-K chunks |
| 6ï¸âƒ£ | ğŸ“‹ Relevant chunks injected into LLM prompt |


## ğŸ”€ Two Modes
| Mode | When | How |
|---|---|---|
| ğŸ¯ **pgvector HNSW** | Default | Semantic similarity search |
| ğŸ“„ **Direct MCP** | Fallback | Full template if RAG fails |


## ğŸ§¬ Embedding Model
- **Provider**: Google Gemini
- **Model**: `text-embedding-004`
- **Dimensions**: 768
- **Index**: HNSW (Hierarchical Navigable Small World)


## ğŸ“š Knowledge Sources
| Source | Content |
|---|---|
| ğŸ”— Chain MCPs | Domain-specific DWH knowledge |
| ğŸ“Š Schema descriptions | Table/column metadata |
| ğŸ‘¥ LDAP hierarchy | User org structure |
| ğŸ“‹ BI catalog | Saved query definitions |
